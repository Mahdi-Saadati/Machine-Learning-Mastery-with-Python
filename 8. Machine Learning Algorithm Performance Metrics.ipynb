{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh/EjOXJdqVu/hUsHPGIue",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahdi-Saadati/Machine-Learning-Mastery-with-Python/blob/main/8.%20Machine%20Learning%20Algorithm%20Performance%20Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Algorithm Performance Metrics**\n",
        "\n",
        "The metrics that you choose to evaluate your machine learning algorithms are very important. Choice of metrics influences how the performance of machine learning algorithms is measured and compared. They influences how you weight the importance of different characteristics in the results and your ultimate choice of which algorithm to choose. In this chapter you will discover how to select and use different machine learning performance metrics in Python with scikit-learn.\n",
        "\n",
        "**8.1 Algorithm Evaluation Metrics**\n",
        "\n",
        "In this lesson, various algorithm evaluation metrics are demonstrated for both classification and regression type machine learning problems.\n",
        "\n",
        "•\tFor classification metrics, the Pima Indians onset of diabetes dataset is used as demonstration. This is a binary classification problem where all of the input variables are numeric.\n",
        "\n",
        "•\tFor regression metrics, the Boston House Price dataset is used as demonstration. this is a regression problem where all of the input variables are also numeric.\n",
        "\n",
        "All recipes evaluate the same algorithms, Logistic Regression for classification and Linear Regression for the regression problems. A 10-fold cross-validation test harness is used to demonstrate each metric, because this is the most likely scenario you will use when employing different algorithm evaluation metrics.\n",
        "A caveat in these recipes is the cross_validation.cross_val_score() function used to report the performance in each recipe. It does allow the use of different scoring metrics that will be discussed, but all scores are reported so that they can be sorted in ascending order (largest score is best). Some evaluation metrics (like mean squared error) are naturally descending scores (the smallest score is best) and as such are reported as negative by the cross_validation.cross_val_score() function. This is important to note, because some scores will be reported as negative that by definition can never be negative. I will remind you about this caveat as we work through the lesson.\n",
        "Let's get on with the evaluation metrics.\n",
        "\n",
        "**8.2 Classification Metrics**\n",
        "\n",
        "Classification problems are perhaps the most common type of machine learning problem and as such there is a myriad of metrics that can be used to evaluate predictions for these problems. In this section we will review how to use the following metrics:\n",
        "\n",
        "•\tClassification Accuracy.\n",
        "\n",
        "•\tLogarithmic Loss.\n",
        "\n",
        "•\tArea Under ROC Curve.\n",
        "\n",
        "•\tConfusion Matrix.\n",
        "\n",
        "•\tClassification Report.\n",
        "\n",
        "**8.2.1 Classification Accuracy**\n",
        "\n",
        "Classification accuracy is the number of correct predictions made as a ratio of all predictions made. This is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case. Below is an example of calculating classification accuracy.\n"
      ],
      "metadata": {
        "id": "hZrWzh0BrukD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation Classification Accuracy\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "kfold = KFold(n_splits=10, random_state=7)\n",
        "model = LogisticRegression()\n",
        "scoring = 'accuracy'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print(\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std())"
      ],
      "metadata": {
        "id": "cz_vm7591prS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the ratio is reported. This can be converted into a percentage by multiplying the value by 100, giving an accuracy score of approximately 77% accurate."
      ],
      "metadata": {
        "id": "2kkJfH2-1sYv"
      }
    }
  ]
}